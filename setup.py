# setup_pinecone.py
import os
from pathlib import Path
from dotenv import load_dotenv 
import pinecone 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# Config
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
if not PINECONE_API_KEY:
    raise ValueError("‚ùå PINECONE_API_KEY manquante dans .env")

INDEX_NAME = "mental-health-raft"

# PDFs
pdf_paths = [
    "docs/EmotionalIntelligence.pdf",
    "docs/Managing-Stress-Principles-and-Strategies-for-Health-and-Wellbeing.pdf",
    "docs/the-social-skills-guidebook-fhc-dr-notes.pdf"
]

# V√©rifier les fichiers
for path in pdf_paths:
    if not Path(path).exists():
        raise FileNotFoundError(f"‚ùå Fichier manquant : {path}")

# Charger et d√©couper
print("üìÑ Chargement des PDFs...")
documents = []
for path in pdf_paths:
    loader = PyPDFLoader(path)
    docs = loader.load()
    documents.extend(docs)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(documents)
print(f"‚úÖ {len(texts)} chunks cr√©√©s.")

# Embeddings 384D
print("üß† G√©n√©ration des embeddings...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Pinecone
print("üì° Connexion √† Pinecone...")
pc = Pinecone(api_key=PINECONE_API_KEY)

# Cr√©er l'index (384D)
if INDEX_NAME not in pc.list_indexes().names():
    pc.create_index(
        name=INDEX_NAME,
        dimension=384,  # ‚úÖ Doit √™tre 384
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-west-1")  # ‚úÖ pas de 'environment'
    )
    print(f"üÜï Index '{INDEX_NAME}' cr√©√© en 384D.")
else:
    print(f"üì• Index '{INDEX_NAME}' existe d√©j√†.")

# Upsert en lots
index = pc.Index(INDEX_NAME)
batch_size = 100
print("üì§ Insertion dans Pinecone...")
for i in range(0, len(texts), batch_size):
    batch = texts[i:i + batch_size]
    contents = [doc.page_content for doc in batch]
    embeds = embeddings.embed_documents(contents)
    vectors = [
        (str(i + j), emb, {"text": contents[j]})
        for j, emb in enumerate(embeds)
    ]
    index.upsert(vectors=vectors)

print(f"‚úÖ {len(texts)} chunks index√©s.")