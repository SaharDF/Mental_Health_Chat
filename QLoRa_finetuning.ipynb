{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7nzHeaJfiyP"
      },
      "source": [
        "## QLoRa Finetuning of LLama-2-2b on the RAFT generated dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XggSgkLefzGf"
      },
      "source": [
        "The fine-tuning process was implemented using QLoRA for memory-efficient training on the RAFT dataset. Using 4-bit quantization and LoRA adapters allowed for fine-tuning LLaMA-2-7B despite GPU memory constraints.\n",
        "\n",
        "The training implementation and hyperparameters were informed by the QLoRA paper's recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pjGb-JL_PXIA",
        "outputId": "b079f7a3-c366-436e-ab2b-41a0e1f0aee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, trl\n",
            "Successfully installed bitsandbytes-0.48.2 trl-0.25.1\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.9.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
            "Failed to build flash-attn\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision datasets transformers tokenizers bitsandbytes peft accelerate trl\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckySJPz_SILX"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNe8ihTrSVfK"
      },
      "outputs": [],
      "source": [
        "# see: https://huggingface.co/docs/hub/security-tokens\n",
        "# must be write token to push model later\n",
        "hf_token = \"hfxxxxxxx\"\n",
        "\n",
        "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# name for output model\n",
        "target_model = \"sahdhif/Llama-2-7b-chat-hf-mental-health\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XriJvPUFUtor"
      },
      "outputs": [],
      "source": [
        "def get_base_prompt():\n",
        "    return \"\"\"\n",
        "    You are a knowledgeable and supportive psychologist. You provide emphatic, non-judgmental responses to users seeking\n",
        "    emotional and psychological support. Provide a safe space for users to share and reflect, focus on empathy, active\n",
        "    listening and understanding.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NBojC9oUzEk"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(input_dict):\n",
        "    \"\"\"\n",
        "    Preprocess the input dictionary to be in the required format.\n",
        "\n",
        "    Args:\n",
        "    input_dict (dict): The input dictionary to be preprocessed\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text in the required format\n",
        "    \"\"\"\n",
        "    # Extract messages from the input dictionary\n",
        "    messages = input_dict['messages']\n",
        "\n",
        "    # Extract the system message\n",
        "    system_message = next(msg['content'] for msg in messages if msg['role'] == 'system')\n",
        "\n",
        "    # Extract the user message\n",
        "    user_message = next(msg['content'] for msg in messages if msg['role'] == 'user')\n",
        "\n",
        "    # Extract the assistant message\n",
        "    assistant_message = next(msg['content'] for msg in messages if msg['role'] == 'assistant')\n",
        "\n",
        "    # Construct the output in the required format\n",
        "    output = f\"### System: {system_message}\\n\\n### User: {user_message}\\n\\n### Assistant: {assistant_message}\"\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "01b72JJymFkM",
        "outputId": "325702ba-6f5a-40ef-d512-15e0492e942c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './output.jsonl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3312375111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output.jsonl'"
          ]
        }
      ],
      "source": [
        "with open('./output.jsonl', 'r') as json_file:\n",
        "    dataset = list(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "16VVVMsKJDHK",
        "outputId": "a353878f-4533-4ecb-b213-ef9e160ae8bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Fichier './output.jsonl' introuvable.\n",
            "üì§ Veuillez uploader votre fichier JSONL (ex: output.jsonl)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2dfbe904-8f78-4d1f-bf5c-58559b9e5d5d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2dfbe904-8f78-4d1f-bf5c-58559b9e5d5d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving output.jsonl to output.jsonl\n",
            "‚úÖ Fichier renomm√© et pr√™t : ./output.jsonl\n",
            "üìñ Chargement du dataset JSONL...\n",
            "‚úÖ 6034 exemples charg√©s.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Nom du fichier cible\n",
        "jsonl_path = \"./output.jsonl\"\n",
        "\n",
        "# 1. V√©rifier si le fichier existe\n",
        "if not os.path.exists(jsonl_path):\n",
        "    print(f\"üìÅ Fichier '{jsonl_path}' introuvable.\")\n",
        "\n",
        "    # 2. Essayer d'uploader (Google Colab)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"üì§ Veuillez uploader votre fichier JSONL (ex: output.jsonl)...\")\n",
        "        uploaded = files.upload()  # Ouvre une fen√™tre d'upload\n",
        "\n",
        "        # S'assurer qu'on a bien upload√© un fichier .jsonl\n",
        "        if uploaded:\n",
        "            # Utiliser le premier fichier upload√©\n",
        "            original_name = list(uploaded.keys())[0]\n",
        "            # Renommer en output.jsonl pour simplifier la suite\n",
        "            os.rename(original_name, \"output.jsonl\")\n",
        "            jsonl_path = \"./output.jsonl\"\n",
        "            print(f\"‚úÖ Fichier renomm√© et pr√™t : {jsonl_path}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Aucun fichier upload√©.\")\n",
        "\n",
        "    except ImportError:\n",
        "        # Pas dans Colab ‚Üí on est en local\n",
        "        raise FileNotFoundError(\n",
        "            \"Veuillez placer 'output.jsonl' dans le m√™me dossier que ce notebook.\"\n",
        "        )\n",
        "\n",
        "# 3. Charger le dataset JSONL\n",
        "print(\"üìñ Chargement du dataset JSONL...\")\n",
        "dataset = []\n",
        "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # Ignorer les lignes vides\n",
        "            dataset.append(json.loads(line))\n",
        "\n",
        "print(f\"‚úÖ {len(dataset)} exemples charg√©s.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GixZjfk8l5B1",
        "outputId": "41549f34-d152-4887-def3-9c1eff446db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"content\": \"You are a knowledgeable and supportive psychologist. You provide emphatic, non-judgmental responses to users seeking\\n    emotional and psychological support. Provide a safe space for users to share and reflect, focus on empathy, active\\n    listening and understanding\",\n",
            "      \"role\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"<DOCUMENT>I‚Äôm ready to let you go. BOX 14.1\\n What the Professor Really Means\\nSchismogenesis: A term coined by Deborah Tannen \\nsuggesting that exaggerated conversation styles become intensiÔ¨Å  ed under stress, thus adding to miscommunication. Metamessages: The underlying intention of verbal \\ncommunication when people are indirect with their comments, thus adding to miscommunication.Reprinted by permission of J.</DOCUMENT>\\n<DOCUMENT>The gratitude showed; the sparkle in her eyes said it all. Behavior ModiÔ¨Å  cation\\n223\\n56147_CH09_216_228.indd   22356147_CH09_216_228.indd   223 9/29/08   11:06:18 PM9/29/08   11:06:18 PMother in times of need. Over time, this ethic has \\nbecome warped so that individuals put other peo-ple‚Äôs needs before their own (e.g., codependent personality). Saying no is mistakenly equated with rudeness, and doing so results in feelings of rejec-tion in the other person. But saying yes when it is inconvenient or impossible results in resentment and victimization in oneself. Assertive ness train-ing teaches people to say no without feeling guilty about hurting someone else‚Äôs feelings. People have the right to refuse a request without harboring feelings of guilt.</DOCUMENT>\\n<DOCUMENT>That will depend on how anxi ous you are\\nand how intensely you‚Äôre exerting yourself. Listen to your bo dy and stop\\nwhen you feel yourself calming down. Channel your anxious ener gy into something useful\\nSometimes you ‚Äôll get really anxious and worked up, and even if you\\nultimately handl e it well, you may still feel amped up and on edge for a\\nlittle while. Getting nervous triggers chemicals in your body and puts it into\\nan alert state; sometimes it takes  a while for all systems to return  to normal. Some people take the attitude of , ‚ÄúW ell, if I have all this extra ener gy , I may\\nas w ell u se it,‚Äù and then clean t heir bathroom or or ganize their apartment‚Äô s\\nstorage locker . Coping with nerves befor e an upcoming social event\\nAnticipating an upcoming socia l situation can cause you to feel anxious. The type s that tend to be the scariest are the ones you know you can‚Äô t back\\nout of.</DOCUMENT>\\n3. What is the purpose of assertiveness training in this context?\",\n",
            "      \"role\": \"user\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"To answer the question about the purpose of assertiveness training in this context, we need to analyze the provided information. \\n\\n1. **Identify the issue**: The context highlights how individuals tend to prioritize other people's needs over their own, leading to feelings of resentment and victimization when they agree to things they cannot or should not do.\\n\\n2. **Purpose of assertiveness training**: The purpose of assertiveness training in this context is to empower individuals to set boundaries and prioritize their own needs without feeling guilty. It teaches people how to say no assertively without sacrificing their own well-being or feeling obligated to please others at the expense of their own happiness.\\n\\n3. **Key points from the context**:\\n   - \\\"Assertiveness training teaches people to say no without feeling guilty about hurting someone else‚Äôs feelings.\\\"\\n   - \\\"People have the right to refuse a request without harboring feelings of guilt.\\\"\\n\\nTherefore, **the purpose of assertiveness training in this context is to help individuals assert their boundaries, prioritize their own needs, and say no without feeling guilty or sacrificing their well-being**.\\n\\n<ANSWER>: The purpose of assertiveness training in this context is to help individuals assert their boundaries, prioritize their own needs, and say no without feeling guilty or sacrificing their well-being.\",\n",
            "      \"role\": \"assistant\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "print(json.dumps(dataset[400], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NClJtX5uuKe"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(datapoint):\n",
        "    \"\"\"\n",
        "    Extrait le message utilisateur √† partir d'une conversation format√©e.\n",
        "    \"\"\"\n",
        "    if \"messages\" not in datapoint:\n",
        "        raise KeyError(\"Champ 'messages' absent dans le datapoint\")\n",
        "\n",
        "    messages = datapoint[\"messages\"]\n",
        "\n",
        "    # Trouver le premier message avec role == \"user\"\n",
        "    user_message = None\n",
        "    for msg in messages:\n",
        "        if msg.get(\"role\") == \"user\":\n",
        "            user_message = msg.get(\"content\", \"\").strip()\n",
        "            break\n",
        "\n",
        "    # Si aucun message utilisateur trouv√©, utiliser le dernier message non-system\n",
        "    if not user_message:\n",
        "        for msg in reversed(messages):\n",
        "            if msg.get(\"role\") != \"system\":\n",
        "                user_message = msg.get(\"content\", \"\").strip()\n",
        "                break\n",
        "\n",
        "    # Si toujours rien, prendre le premier contenu\n",
        "    if not user_message:\n",
        "        user_message = messages[0].get(\"content\", \"\").strip() if messages else \"\"\n",
        "\n",
        "    # Nettoyage basique (optionnel)\n",
        "    user_message = user_message.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    return {\n",
        "        \"text\": user_message,\n",
        "        \"label\": datapoint.get(\"label\", \"unknown\"),  # garde le label si pr√©sent\n",
        "        # Tu peux ajouter d'autres m√©tadonn√©es si besoin\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbd7C6GcuyLg",
        "outputId": "16356e58-3cb3-41c1-fe5b-b54c3ef144a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6034/6034 [00:00<00:00, 12159.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset preprocessing complete. Output saved to processed_outputs.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Assure-toi que la fonction preprocess_text est bien d√©finie (voir ci-dessous)\n",
        "def preprocess_text(datapoint):\n",
        "    if \"messages\" not in datapoint:\n",
        "        raise KeyError(\"Champ 'messages' absent dans le datapoint\")\n",
        "\n",
        "    messages = datapoint[\"messages\"]\n",
        "    user_message = None\n",
        "\n",
        "    # Cherche le message de l'utilisateur\n",
        "    for msg in messages:\n",
        "        if msg.get(\"role\") == \"user\":\n",
        "            user_message = msg.get(\"content\", \"\").strip()\n",
        "            break\n",
        "\n",
        "    # Fallback : dernier message non system\n",
        "    if not user_message:\n",
        "        for msg in reversed(messages):\n",
        "            if msg.get(\"role\") != \"system\":\n",
        "                user_message = msg.get(\"content\", \"\").strip()\n",
        "                break\n",
        "\n",
        "    # Dernier recours\n",
        "    if not user_message and messages:\n",
        "        user_message = messages[0].get(\"content\", \"\").strip()\n",
        "\n",
        "    user_message = user_message.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    return {\n",
        "        \"text\": user_message,\n",
        "        \"label\": datapoint.get(\"label\", \"unknown\")\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# APPEL CORRIG√â DE LA FONCTION\n",
        "# -----------------------------\n",
        "preprocess_dataset_to_jsonl(\n",
        "    input_dataset=dataset,\n",
        "    output_file='processed_outputs.jsonl',\n",
        "    preprocess_text_func=preprocess_text  # ‚ö†Ô∏è Nom du param√®tre correct\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset preprocessing complete. Output saved to processed_outputs.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcJsbWT4w41u"
      },
      "outputs": [],
      "source": [
        "with open('./processed_outputs.jsonl', 'r') as json_file:\n",
        "    dataset2 = list(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1R3cV2fxAOX",
        "outputId": "0257f149-9a1f-4464-c680-03b4192399ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '<DOCUMENT>Wrong. Multi-tasking\\tactually\\tsacrifices\\tyour\\tquality\\tof\\twork,\\tas\\tthe\\tbrain\\tis\\tsimply incapable\\tof\\tperforming\\tat\\ta\\thigh\\tlevel\\tin\\tmultiple\\tactivities\\tat\\tonce. Let‚Äôs\\tsay\\tyou‚Äôre\\tin\\ta\\tmeeting\\twhere\\tseveral\\tideas\\tare\\tbeing\\tshared.</DOCUMENT> <DOCUMENT>Maybe one or two coworkers aren‚Äô t fans of yours, but most are probably pretty neutral about you. ‚ÄúIf I go out to the bar with my friends, I know all kinds of annoying things will go wr ong with the night.‚Äù (Fortune-telling) Alternative:  Soc ial events hardly ever turn  out exactly as we predict or anticipate, good or bad. The more social experience you get, the more this point will be driven home. ‚ÄúI can‚Äôt see myself becoming extr emely charismatic so I don‚Äôt see the point in working on my people skills.‚Äù (Black-and-white thinking) Alternative:  Even tweaking your social sk ills a little can make a big dif ference in the quality of your life. Y ou only need average peo ple skills toenjoy most of what the social world has to of fer . ‚ÄúNot everyone in my class likes me. That means I‚Äôm a complete r eject.‚Äù (Black-and-white thinking) Alternative:  Y our worth as a person doesn‚Äô t hinge on having every last person like you. No one is universally liked.</DOCUMENT> <DOCUMENT>Accept that you may not be fu lly r eady to change yet. Regardless of h ow you thin k you should  f eel , your heart  may not fully be in it at the mom ent. If you don‚Äô t have an inner drive to tackle your issue s, no rah-rah speech or quote is going  to fix that. At best that‚Äôll mak e you feel psyched up for a day or two  before you go back to the status  quo. There‚Äô s nothing wrong with deciding to wait until a greater sense of ur gency sets in. Set aside some time to figur e out what you r eally want. Are you telling y ourself you should try to become an outgoing party animal because that‚Äô s what society say s is important, when deep do wn it doesn‚Äô t interest you? W ould yo u feel more enthusiastic about trying to develop a more low-key social life? Realize the hardest part is often getting started.</DOCUMENT> 1. Can social events always turn out as we predict or anticipate?', 'label': 'unknown'}\n"
          ]
        }
      ],
      "source": [
        "print(json.loads(dataset2[40]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiJh0DhLUzzt"
      },
      "outputs": [],
      "source": [
        "def train_mental_health_model():\n",
        "    # Check if CUDA is available and the GPU is compatible with FlashAttention\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        if not any(x in gpu_name for x in [\"A100\", \"RTX 30\", \"RTX 40\", \"H100\"]):  # Check for Ampere or newer GPUs\n",
        "            print(f\"Warning: Your GPU ({gpu_name}) might not be fully compatible with FlashAttention. \"\n",
        "                  f\"Consider disabling FlashAttention for optimal performance.\")\n",
        "            attn_implementation = None  # Disable FlashAttention\n",
        "        else:\n",
        "            attn_implementation = \"flash_attention_2\"  # Enable FlashAttention\n",
        "    else:\n",
        "        attn_implementation = None  # Disable FlashAttention if no CUDA is available\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        token=hf_token,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=False\n",
        "        ),\n",
        "        torch_dtype=torch.float16,  # reduce memory usage\n",
        "        attn_implementation=attn_implementation  # optimize for tensor cores (NVIDIA A100)\n",
        "    )\n",
        "\n",
        "    # LoRA config based on QLoRA paper\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        r=8,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=target_model,  # model output directory\n",
        "        overwrite_output_dir=True,  # overwrite output if exists\n",
        "        num_train_epochs=2,  # number of epochs to train 3 to 5 epochs\n",
        "        per_device_train_batch_size=2,  # batch size per device during training\n",
        "        gradient_checkpointing=True,  # save memory but causes slower training\n",
        "        logging_steps=10,  # log every 10 steps\n",
        "        learning_rate=1e-4,  # learning rate\n",
        "        max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
        "        warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
        "        optim=\"paged_adamw_8bit\",  # memory-efficient variant of AdamW optimizer\n",
        "        lr_scheduler_type=\"constant\",  # constant learning rate\n",
        "        save_strategy=\"epoch\",  # save at the end of each epoch\n",
        "        evaluation_strategy=\"epoch\",  # evaluation at the end of each epoch,\n",
        "        fp16=True,  # use fp16 16-bitprecision training instead of 32-bit to save memory\n",
        "        #tf32=True  # optimize for tensor cores (NVIDIA A100)\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model, token=hf_token)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # limit samples to reduce memory usage\n",
        "    dataset = load_dataset(\"json\", data_files=\"output.jsonl\", split=\"train\")\n",
        "    train_dataset = dataset.select(range(2000))\n",
        "    eval_dataset = dataset.select(range(2000, 2500))\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=1024,\n",
        "        tokenizer=tokenizer,\n",
        "        packing=True,\n",
        "        args=args\n",
        "    )\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    trainer.push_to_hub(target_model, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6U7FTSaMHp0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Remplace par ton vrai token\n",
        "login(token=\"hf_xxxx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3e17464e9e4a475f8dcd66753619ea08",
            "c7f3f5b239e94a48907961914e4ca49f",
            "45b1a26b98524311b9421857197779d1",
            "bd85536defa6490485dd2f0771228981",
            "99f12df972004a17982f3a244c343f61",
            "e9cad4fd0411462b9480fd84eb8cf21f",
            "539319c20983462792e2d38ff05884ea",
            "5ff7ad5f3c69410e8576e211ac10d8e7",
            "ca65397021b646b9a8d2b527eeea277a",
            "9a9e178832f24227a9731aa5544c0a1d",
            "94869fd274104913a5e578f214973b4d",
            "994b864b106e49eb86cb60da797e55f5",
            "a467d3cc93d049198040e08004379439",
            "7535f1ad60b744a7bde06dd6afeb8568",
            "2a76932277334826bf337e8a47c99580",
            "4622bcfdfa2341a48b224a4bf7585553",
            "5a9974eb00d542fdbb2645547cb6c9d9",
            "85f0f496e9cb43dabfa2e05a18ca7328",
            "c643f16cee1940c2b43adc0d0568d828",
            "1499ac408b3b4e2f84234c70ccb1448c"
          ]
        },
        "id": "CMiiDZeuNd-O",
        "outputId": "49aff592-970f-4829-dccc-39b3652e590e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e17464e9e4a475f8dcd66753619ea08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D9IKGAhnN5oO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "    print(\"‚úÖ Tokenizer charg√© avec succ√®s !\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Erreur :\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qmo4F9GBmQUc"
      },
      "outputs": [],
      "source": [
        "train_mental_health_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1499ac408b3b4e2f84234c70ccb1448c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a76932277334826bf337e8a47c99580": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3e17464e9e4a475f8dcd66753619ea08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_539319c20983462792e2d38ff05884ea"
          }
        },
        "45b1a26b98524311b9421857197779d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9a9e178832f24227a9731aa5544c0a1d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_94869fd274104913a5e578f214973b4d",
            "value": ""
          }
        },
        "4622bcfdfa2341a48b224a4bf7585553": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539319c20983462792e2d38ff05884ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5a9974eb00d542fdbb2645547cb6c9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ff7ad5f3c69410e8576e211ac10d8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7535f1ad60b744a7bde06dd6afeb8568": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f0f496e9cb43dabfa2e05a18ca7328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c643f16cee1940c2b43adc0d0568d828",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1499ac408b3b4e2f84234c70ccb1448c",
            "value": "Connecting..."
          }
        },
        "94869fd274104913a5e578f214973b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "994b864b106e49eb86cb60da797e55f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f12df972004a17982f3a244c343f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7535f1ad60b744a7bde06dd6afeb8568",
            "style": "IPY_MODEL_2a76932277334826bf337e8a47c99580",
            "tooltip": ""
          }
        },
        "9a9e178832f24227a9731aa5544c0a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a467d3cc93d049198040e08004379439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd85536defa6490485dd2f0771228981": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_994b864b106e49eb86cb60da797e55f5",
            "style": "IPY_MODEL_a467d3cc93d049198040e08004379439",
            "value": true
          }
        },
        "c643f16cee1940c2b43adc0d0568d828": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f3f5b239e94a48907961914e4ca49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ff7ad5f3c69410e8576e211ac10d8e7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca65397021b646b9a8d2b527eeea277a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ca65397021b646b9a8d2b527eeea277a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9cad4fd0411462b9480fd84eb8cf21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4622bcfdfa2341a48b224a4bf7585553",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a9974eb00d542fdbb2645547cb6c9d9",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
